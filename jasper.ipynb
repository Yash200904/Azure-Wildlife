{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team: Wildland Fire Azure AI\n",
    "## Project: Jasper Fire Recovery - Bridging Technology and Disaster Recover "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "###### In this project, we explore how technology, specifically geospatial data and automated workflows, can be used to support disaster recovery efforts. The Jasper Fire serves as a real-world case study for testing our approach. The goal is to show how leveraging code and spatial data can enhance situational awareness, optimize resource allocation, and ultimately learn how natural environments can be restored to their pre-disaster states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview: \n",
    "###### This project investigates the recovery of the 2000 Jasper Fire in the Black Hills of South Dakota. Leveraging Landsat satellite imagery, machine learning models, and Azure Machine Learning Studio, we analyze the ecological recovery and explore actionable insights for forest management and wildfire resilience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Jasper Fire Burn Scar](https://firerecovery.blob.core.windows.net/azureml-blobstore-d516a889-2242-4cd1-9ee4-4a3315f1782b/UI/2024-11-30_185318_UTC/391249586-31db6714-f3a1-4f44-b377-8fb557f8d2e9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "###### * Utilize Landsat imagery to assess post-fire recovery.\n",
    "###### * Analyze ecological metrics like burn severity and vegetation recovery. \n",
    "###### * Develop machine learning workflows for geospatial data analysis. \n",
    "###### * Highlight ongoing restoration efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "###### The Jasper Fire, ignited in August 2000, burned over 83,000 acres in the Black Hills National Forest. This catastrophic event led to significant ecological damage and has required extensive restoration efforts. \n",
    "\n",
    "#### Challenges of Recovery \n",
    "\n",
    "###### * Manual replanting efforts are limited to a 2-week window each year.\n",
    "###### * Replanting focuses on native Ponderosa Pine at a rate of 400 acres per year.\n",
    "###### * Increased wildfire frequency due to climate change poses ongoing challenges.\n",
    "\n",
    "#### Solution: Remote Sensing & AI/ML for Wildfire Recovery \n",
    "###### \n",
    "###### We developed a prototype solution using geospatial analysis and AI/ML tools within Azure ML Studio to:\n",
    "###### * Track vegetation recovery\n",
    "###### * Assess land use changes\n",
    "###### * Guide future fire resilience efforts\n",
    "\n",
    "#### Details of the Solution\n",
    "###### * **Data Storage:** Azure Storage Blob for scalable data storage.\n",
    "###### * **Geospatial Analysis:**  Landsat imagery to monitor vegetation and land use.\n",
    "###### * **Python Notebooks:**  Automate data analysis and visualization.\n",
    "###### * **AI/ML Models:**  Azure ML Studio to predict fire-prone areas and track vegetation health.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In order to run this script you will need to run Python 3.8 and the following libraries: \n",
    "###### rasterio \n",
    "###### matplotlib \n",
    "###### numpy \n",
    "###### scikit-learn\n",
    "###### pandas\n",
    "###### pygments\n",
    "###### azureml-core \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute and Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Make sure your kernel is Python 3.8 - AzureML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n azureml_py38 -c conda-forge python=3.8 -y\n",
    "!conda activate azureml_py38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from osgeo import gdal\n",
    "\n",
    "# Import other essential libraries for your analysis\n",
    "# For example:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "# Connect to your workspace\n",
    "ws = Workspace.from_config()  # This assumes you have a config file (config.json)\n",
    "\n",
    "# Access your data using relative paths\n",
    "dataset = Dataset.File.from_files(path=[\n",
    "    (ws.datastores['workspaceblobstore'], 'UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/'), \n",
    "    (ws.datastores['workspaceblobstore'], 'UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001//'), \n",
    "])\n",
    "\n",
    "# Explore the dataset\n",
    "dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# Check for specific packages and their versions\n",
    "try:\n",
    "    import numpy\n",
    "    print(\"NumPy version:\", numpy.__version__)\n",
    "except ImportError:\n",
    "    print(\"NumPy not found.\")\n",
    "\n",
    "try:\n",
    "    import pandas\n",
    "    print(\"Pandas version:\", pandas.__version__)\n",
    "except ImportError:\n",
    "    print(\"Pandas not found.\")\n",
    "\n",
    "# Add more checks for other packages you need (e.g., matplotlib, scikit-learn, gdal, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Landsat imagery using rasterio\n",
    "with rasterio.open('path/to/landsat/image.tif') as src:\n",
    "    landsat_data = src.read()\n",
    "    landsat_meta = src.meta\n",
    "\n",
    "# Perform band calculations using NumPy\n",
    "ndvi = (landsat_data[4] - landsat_data[3]) / (landsat_data[4] + landsat_data[3])\n",
    "\n",
    "# Visualize NDVI using matplotlib\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(ndvi, cmap='RdYlGn')\n",
    "plt.title('NDVI from Landsat Imagery')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Landsat Imagery Data Sets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection\n",
    "###### We used the following workflow to acquire and preprocess data:\n",
    "###### Earth Explorer: Landsat satellite imagery of the Jasper Fire region was accessed through the USGS Earth Explorer platform.\n",
    "###### ESPA: The imagery was downloaded in GeoTIFF format in two batches:\n",
    "###### jasper-landsat-130\n",
    "###### jasper-landsat-054\n",
    "###### Azure Storage Blob: The TIFF files were uploaded to Azure Storage Blob for collaborative access by the team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Landsat Data sourced from Earth Explorer stored in Azure Storage Blob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landsat 130 Paths\n",
    "landsat_130_ui_path = \"UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/\"\n",
    "landsat_130_azureml_path = \"azureml://subscriptions/6328cbe3-a1c5-406e-a25d-72120ce95fdf/resourcegroups/MSLA/workspaces/jasper/datastores/workspaceblobstore/paths/UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/\"\n",
    "landsat_130_blob_url = \"https://firerecovery.blob.core.windows.net/azureml-blobstore-d516a889-2242-4cd1-9ee4-4a3315f1782b/UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/\"\n",
    "\n",
    "# Landsat 054 Paths\n",
    "landsat_054_ui_path = \"UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001/\"\n",
    "landsat_054_azureml_path = \"azureml://subscriptions/6328cbe3-a1c5-406e-a25d-72120ce95fdf/resourcegroups/MSLA/workspaces/jasper/datastores/workspaceblobstore/paths/UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001/\"\n",
    "landsat_054_blob_url = \"https://firerecovery.blob.core.windows.net/azureml-blobstore-d516a889-2242-4cd1-9ee4-4a3315f1782b/UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "# Load the workspace configuration\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Access the default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Define the paths to your datasets\n",
    "landsat_054_path = \"UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001/\"\n",
    "landsat_130_path = \"UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/\"\n",
    "\n",
    "# Create datasets for the paths\n",
    "landsat_054_dataset = Dataset.File.from_files((datastore, landsat_054_path))\n",
    "landsat_130_dataset = Dataset.File.from_files((datastore, landsat_130_path))\n",
    "\n",
    "# List the files in the datasets\n",
    "print(\"Files in Landsat 054 dataset:\")\n",
    "for file_path in landsat_054_dataset.to_path():\n",
    "    print(file_path)\n",
    "\n",
    "print(\"\\nFiles in Landsat 130 dataset:\")\n",
    "for file_path in landsat_130_dataset.to_path():\n",
    "    print(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Jasper Fire Area Landsat Data Exploration and Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study Area - Jasper Fire within Black Hills National Forest\n",
    "\n",
    "![Black Hills National Forest](https://firerecovery.blob.core.windows.net/azureml-blobstore-d516a889-2242-4cd1-9ee4-4a3315f1782b/UI/2024-11-30_192011_UTC/Screenshot%202024-11-30%20111812.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burn Boundary and Land Markers \n",
    "\n",
    "#### The burn scar is in the south west corner of the Black Hills National Forest\n",
    "\n",
    "##### Burn Scar from NBR Images 2020 \n",
    "\n",
    "![Burn Scar from NBR Images in Black and White](https://firerecovery.blob.core.windows.net/azureml-blobstore-d516a889-2242-4cd1-9ee4-4a3315f1782b/UI/2024-11-30_192011_UTC/photos/photos/Screenshot%202024-11-30%20144038.png)\n",
    "\n",
    "#### The burn scar remains 20 years after the wildfire\n",
    "\n",
    "##### General Context of Fire Location \n",
    "![Jasper Fire Burn Boundary](https://firerecovery.blob.core.windows.net/azureml-blobstore-d516a889-2242-4cd1-9ee4-4a3315f1782b/UI/2024-11-30_213818_UTC/Jasper3.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Analysis\n",
    "###### Burn Severity Mapping: Using indices like dNBR (Differenced Normalized Burn Ratio) to measure the severity of the fire’s impact on vegetation.\n",
    "###### Vegetation Recovery: Using NDVI (Normalized Difference Vegetation Index) to track regrowth over the past 25 years.\n",
    "######\n",
    "###### There are many possibilities of what can be done with this data and that includes future analysis such as: \n",
    "###### Fuel Load Modeling: Leveraging 3D geospatial data, including LiDAR, to model fuel loads and better understand forest recovery dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Burn Index (NBR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "# Load the workspace configuration\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Access the default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Define the paths to your datasets\n",
    "landsat_054_path = \"UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001/\"\n",
    "landsat_130_path = \"UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/\"\n",
    "\n",
    "# Create datasets for the paths\n",
    "landsat_054_dataset = Dataset.File.from_files((datastore, landsat_054_path))\n",
    "landsat_130_dataset = Dataset.File.from_files((datastore, landsat_130_path))\n",
    "\n",
    "# Helper function to filter and sort files\n",
    "def filter_and_sort_files(dataset):\n",
    "    file_list = dataset.to_path()\n",
    "    \n",
    "    # Filter files containing \"NBR\" in their names\n",
    "    filtered_files = [file_path for file_path in file_list if \"NBR\" in file_path]\n",
    "    \n",
    "    # Extract year and month from the filenames and sort\n",
    "    sorted_files = sorted(\n",
    "        filtered_files,\n",
    "        key=lambda x: (int(x.split(\"_\")[3][:4]), int(x.split(\"_\")[3][4:6]))  # Extract year and month\n",
    "    )\n",
    "    \n",
    "    return sorted_files\n",
    "\n",
    "# Apply the helper function to both datasets\n",
    "sorted_landsat_054_files = filter_and_sort_files(landsat_054_dataset)\n",
    "sorted_landsat_130_files = filter_and_sort_files(landsat_130_dataset)\n",
    "\n",
    "# Print the sorted files\n",
    "print(\"Filtered and sorted files in Landsat 054 dataset:\")\n",
    "for file_path in sorted_landsat_054_files:\n",
    "    print(file_path)\n",
    "\n",
    "print(\"\\nFiltered and sorted files in Landsat 130 dataset:\")\n",
    "for file_path in sorted_landsat_130_files:\n",
    "    print(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBR (Normalized Burn Ratio)\n",
    "##### Image from 2023\n",
    "![NRB in 2023](https://firerecovery.blob.core.windows.net/azureml-blobstore-d516a889-2242-4cd1-9ee4-4a3315f1782b/UI/2024-11-30_220544_UTC/Screenshot%202024-11-30%20135822.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "# Load the workspace configuration\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Access the default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Define the paths to your datasets\n",
    "landsat_054_path = \"UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001/\"\n",
    "landsat_130_path = \"UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/\"\n",
    "\n",
    "# Create datasets for the paths\n",
    "landsat_054_dataset = Dataset.File.from_files((datastore, landsat_054_path))\n",
    "landsat_130_dataset = Dataset.File.from_files((datastore, landsat_130_path))\n",
    "\n",
    "# Helper function to filter, separate, and sort files\n",
    "def filter_separate_and_sort_files(dataset):\n",
    "    file_list = dataset.to_path()\n",
    "    \n",
    "    # Separate files into NBR and NBR2\n",
    "    nbr_files = [file_path for file_path in file_list if \"NBR\" in file_path and \"NBR2\" not in file_path]\n",
    "    nbr2_files = [file_path for file_path in file_list if \"NBR2\" in file_path]\n",
    "    \n",
    "    # Extract year and month from the filenames and sort each group\n",
    "    sorted_nbr_files = sorted(\n",
    "        nbr_files,\n",
    "        key=lambda x: (int(x.split(\"_\")[3][:4]), int(x.split(\"_\")[3][4:6]))  # Extract year and month\n",
    "    )\n",
    "    sorted_nbr2_files = sorted(\n",
    "        nbr2_files,\n",
    "        key=lambda x: (int(x.split(\"_\")[3][:4]), int(x.split(\"_\")[3][4:6]))  # Extract year and month\n",
    "    )\n",
    "    \n",
    "    return sorted_nbr_files, sorted_nbr2_files\n",
    "\n",
    "# Apply the helper function to both datasets\n",
    "nbr_files_054, nbr2_files_054 = filter_separate_and_sort_files(landsat_054_dataset)\n",
    "nbr_files_130, nbr2_files_130 = filter_separate_and_sort_files(landsat_130_dataset)\n",
    "\n",
    "# Print the results\n",
    "print(\"Filtered and sorted NBR files in Landsat 054 dataset:\")\n",
    "for file_path in nbr_files_054:\n",
    "    print(file_path)\n",
    "\n",
    "print(\"\\nFiltered and sorted NBR2 files in Landsat 054 dataset:\")\n",
    "for file_path in nbr2_files_054:\n",
    "    print(file_path)\n",
    "\n",
    "print(\"\\nFiltered and sorted NBR files in Landsat 130 dataset:\")\n",
    "for file_path in nbr_files_130:\n",
    "    print(file_path)\n",
    "\n",
    "print(\"\\nFiltered and sorted NBR2 files in Landsat 130 dataset:\")\n",
    "for file_path in nbr2_files_130:\n",
    "    print(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "# Load the workspace configuration\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Access the default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Define the paths to your datasets\n",
    "landsat_054_path = \"UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001/\"\n",
    "landsat_130_path = \"UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/\"\n",
    "\n",
    "# Create datasets for the paths\n",
    "landsat_054_dataset = Dataset.File.from_files((datastore, landsat_054_path))\n",
    "landsat_130_dataset = Dataset.File.from_files((datastore, landsat_130_path))\n",
    "\n",
    "# ... (rest of your code)\n",
    "\n",
    "# Apply the helper function to both datasets\n",
    "nbr_files_054, nbr2_files_054 = filter_separate_and_sort_files(landsat_054_dataset)\n",
    "nbr_files_130, nbr2_files_130 = filter_separate_and_sort_files(landsat_130_dataset)\n",
    "\n",
    "# Combine the file paths\n",
    "all_nbr_files = nbr_files_054 + nbr_files_130  # Concatenate the lists\n",
    "\n",
    "# Print the results\n",
    "print(\"All NBR files:\")\n",
    "for file_path in all_nbr_files:\n",
    "    print(file_path)\n",
    "\n",
    "# ... (rest of your code to visualize the NBR data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composite of Multiple Rasters (July 2020 and July 2024)\n",
    "![Black and White NBR Composite](https://firerecovery.blob.core.windows.net/azureml-blobstore-d516a889-2242-4cd1-9ee4-4a3315f1782b/UI/2024-11-30_192011_UTC/photos/Screenshot%202024-11-30%20142947.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from azureml.core import Workspace, Dataset\n",
    "import rasterio\n",
    "from rasterio.errors import RasterioIOError\n",
    "\n",
    "# Define paths for specific years (adjust as needed)\n",
    "landsat_130_path_2013 = \"UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/*2013*_SR_NBR.tif\"\n",
    "landsat_130_path_2024 = \"UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/*2024*_SR_NBR.tif\"\n",
    "\n",
    "# Create FileDatasets for the specific files\n",
    "landsat_130_dataset_2013 = Dataset.File.from_files((datastore, landsat_130_path_2013))\n",
    "landsat_130_dataset_2024 = Dataset.File.from_files((datastore, landsat_130_path_2024))\n",
    "\n",
    "# Download the datasets to a local directory\n",
    "download_path = './nbr_files'\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "landsat_130_dataset_2013.download(target_path=download_path, overwrite=True)\n",
    "landsat_130_dataset_2024.download(target_path=download_path, overwrite=True)\n",
    "\n",
    "# Get the downloaded file paths\n",
    "downloaded_files = [\n",
    "    os.path.join(download_path, f) for f in os.listdir(download_path) if f.endswith('.tif')\n",
    "]\n",
    "\n",
    "# Visualize the downloaded NBR TIFF files\n",
    "for tiff_file_path in downloaded_files:\n",
    "    try:\n",
    "        with rasterio.open(tiff_file_path) as dataset:\n",
    "            nbr_data = dataset.read(1)  # Read the NBR band\n",
    "\n",
    "            # Visualize the data\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(nbr_data, cmap='viridis')\n",
    "            plt.colorbar(label='Pixel Values')\n",
    "            plt.title(f'NBR Visualization: {tiff_file_path}')\n",
    "            plt.xlabel('Column')\n",
    "            plt.ylabel('Row')\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {tiff_file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.data.data_reference import DataReference\n",
    "import rasterio\n",
    "from rasterio.errors import RasterioIOError\n",
    "\n",
    "# Load workspace and datastore\n",
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Define paths with wildcard for NBR files\n",
    "landsat_130_path = \"UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/*_SR_NBR.tif\"\n",
    "landsat_054_path = \"UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001/*_SR_NBR.tif\"\n",
    "\n",
    "# Create FileDatasets\n",
    "landsat_130_dataset = Dataset.File.from_files((datastore, landsat_130_path))\n",
    "landsat_054_dataset = Dataset.File.from_files((datastore, landsat_054_path))\n",
    "\n",
    "# Helper function to filter NBR TIFF files by year\n",
    "def filter_nbr_files_by_year(dataset):\n",
    "    file_list = dataset.to_path()\n",
    "    nbr_files = [file for file in file_list if \"SR_NBR.tif\" in file]\n",
    "    files_by_year = {}\n",
    "    for file in nbr_files:\n",
    "        year = file.split(\"_\")[3][:4]  # Extract year (YYYY)\n",
    "        if year not in files_by_year:\n",
    "            files_by_year[year] = []\n",
    "        files_by_year[year].append(file)\n",
    "    return files_by_year\n",
    "\n",
    "# Filter NBR files by year\n",
    "nbr_files_130_by_year = filter_nbr_files_by_year(landsat_130_dataset)\n",
    "\n",
    "# Stream and visualize the NBR TIFF files for 2013 and 2024\n",
    "for year, files in nbr_files_130_by_year.items():\n",
    "    if year in ['2013', '2024']:  # Filter for the years you want\n",
    "        print(f\"Visualizing NBR files for year {year}...\")\n",
    "        for file_path in files:\n",
    "            print(f\"  Visualizing: {file_path}\")\n",
    "            try:\n",
    "                # Create a DataReference object\n",
    "                data_reference = DataReference(\n",
    "                    datastore=datastore,\n",
    "                    data_reference_name=\"landsat_data\",\n",
    "                    path_on_datastore=file_path\n",
    "                )\n",
    "\n",
    "                # Access the data using as_download()\n",
    "                downloaded_path = data_reference.as_download()\n",
    "\n",
    "                # Open the downloaded file with rasterio\n",
    "                with rasterio.open(downloaded_path) as dataset:\n",
    "                    nbr_data = dataset.read(1)  # Read the NBR band\n",
    "\n",
    "                    # Visualize the data\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    plt.imshow(nbr_data, cmap='viridis')\n",
    "                    plt.colorbar(label='Pixel Values')\n",
    "                    plt.title(f'NBR Visualization: {file_path}')\n",
    "                    plt.xlabel('Column')\n",
    "                    plt.ylabel('Row')\n",
    "                    plt.show()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.data.data_reference import DataReference\n",
    "import rasterio\n",
    "from rasterio.errors import RasterioIOError\n",
    "\n",
    "# Load workspace and datastore\n",
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Define paths with wildcard for NBR files\n",
    "landsat_130_path = \"UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/*_SR_NBR.tif\"\n",
    "landsat_054_path = \"UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001/*_SR_NBR.tif\"\n",
    "\n",
    "# Create FileDatasets\n",
    "landsat_130_dataset = Dataset.File.from_files((datastore, landsat_130_path))\n",
    "landsat_054_dataset = Dataset.File.from_files((datastore, landsat_054_path))\n",
    "\n",
    "# Helper function to filter NBR TIFF files by year\n",
    "def filter_nbr_files_by_year(dataset):\n",
    "    file_list = dataset.to_path()\n",
    "    nbr_files = [file for file in file_list if \"SR_NBR.tif\" in file]\n",
    "    files_by_year = {}\n",
    "    for file in nbr_files:\n",
    "        year = file.split(\"_\")[3][:4]  # Extract year (YYYY)\n",
    "        if year not in files_by_year:\n",
    "            files_by_year[year] = []\n",
    "        files_by_year[year].append(file)\n",
    "    return files_by_year\n",
    "\n",
    "# Filter NBR files by year\n",
    "nbr_files_130_by_year = filter_nbr_files_by_year(landsat_130_dataset)\n",
    "\n",
    "# Stream and visualize the NBR TIFF files for 2013 and 2024\n",
    "for year, files in nbr_files_130_by_year.items():\n",
    "    if year in ['2013', '2024']:  # Filter for the years you want\n",
    "        print(f\"Visualizing NBR files for year {year}...\")\n",
    "        for file_path in files:\n",
    "            print(f\"  Visualizing: {file_path}\")\n",
    "            try:\n",
    "                # Create a DataReference object\n",
    "                data_reference = DataReference(\n",
    "                    datastore=datastore,\n",
    "                    data_reference_name=\"landsat_data\",\n",
    "                    path_on_datastore=file_path\n",
    "                )\n",
    "\n",
    "                # Open the DataReference as a stream with rasterio\n",
    "                with rasterio.open(data_reference.as_download()) as dataset:\n",
    "                    nbr_data = dataset.read(1)  # Read the NBR band\n",
    "\n",
    "                    # Visualize the data\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    plt.imshow(nbr_data, cmap='viridis')\n",
    "                    plt.colorbar(label='Pixel Values')\n",
    "                    plt.title(f'NBR Visualization: {file_path}')\n",
    "                    plt.xlabel('Column')\n",
    "                    plt.ylabel('Row')\n",
    "                    plt.show()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand Replanting is Essential to Forest Post-Fire Recovery  \n",
    "###### Each year the US Forestry Service has a 2 week window in April to replant nursery grown native Ponderosa Pine throughout the burn area. This effort has continued now for over 20 years. \n",
    "\n",
    "###### This is a visualization of the areas within the Jasper Fire area that have been replanted. \n",
    "\n",
    "![New Trees Planted](https://firerecovery.blob.core.windows.net/azureml-blobstore-d516a889-2242-4cd1-9ee4-4a3315f1782b/UI/2024-11-30_192011_UTC/Screenshot%202024-11-30%20134507.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "import rasterio\n",
    "from rasterio.errors import RasterioIOError  # Import RasterioIOError\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the workspace configuration\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Access the default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Define the paths to your datasets\n",
    "landsat_054_path = \"UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001/\"\n",
    "landsat_130_path = \"UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/\"\n",
    "\n",
    "# Create datasets for the paths\n",
    "landsat_054_dataset = Dataset.File.from_files((datastore, landsat_054_path))\n",
    "landsat_130_dataset = Dataset.File.from_files((datastore, landsat_130_path))\n",
    "\n",
    "# ... (rest of your code)\n",
    "\n",
    "# Apply the helper function to both datasets\n",
    "nbr_files_054, nbr2_files_054 = filter_separate_and_sort_files(landsat_054_dataset)\n",
    "nbr_files_130, nbr2_files_130 = filter_separate_and_sort_files(landsat_130_dataset)\n",
    "\n",
    "# Combine the file paths\n",
    "all_nbr_files = nbr_files_054 + nbr_files_130  # Concatenate the lists\n",
    "\n",
    "# Visualize the NBR data\n",
    "for tiff_file_path in all_nbr_files:\n",
    "    try:\n",
    "        with rasterio.open(tiff_file_path) as dataset:\n",
    "            nbr_data = dataset.read(1)  # Read the NBR data\n",
    "            plt.imshow(nbr_data, cmap='RdYlGn')  # Use a suitable colormap for NBR\n",
    "            plt.colorbar()\n",
    "            plt.title(f'NBR - {os.path.basename(tiff_file_path)}')  # Set title with file name\n",
    "            plt.show()\n",
    "\n",
    "    except RasterioIOError as e:\n",
    "        print(f\"Error opening TIFF file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shows the Growth Over a 4 Year Period Using Change Detection Analysis\n",
    "\n",
    "#### Using Change Detection This Shows Regrowth Which Matches Vector Data for Hand Replanting \n",
    "\n",
    "![Change Detection](https://firerecovery.blob.core.windows.net/azureml-blobstore-d516a889-2242-4cd1-9ee4-4a3315f1782b/UI/2024-11-30_192011_UTC/photos/photos/photos/Screenshot%202024-11-30%20145349.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.data.data_reference import DataReference\n",
    "import rasterio\n",
    "from rasterio.errors import RasterioIOError\n",
    "\n",
    "# Load workspace and datastore (as before)\n",
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Define paths (as before)\n",
    "landsat_130_path = \"UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/*_SR_NBR.tif\"\n",
    "landsat_054_path = \"UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001/*_SR_NBR.tif\"\n",
    "\n",
    "# Create FileDatasets (as before)\n",
    "landsat_130_dataset = Dataset.File.from_files((datastore, landsat_130_path))\n",
    "landsat_054_dataset = Dataset.File.from_files((datastore, landsat_054_path))\n",
    "\n",
    "# ... (your filter_nbr_files_by_year function)\n",
    "\n",
    "# Filter NBR files by year\n",
    "nbr_files_130_by_year = filter_nbr_files_by_year(landsat_130_dataset)\n",
    "\n",
    "# Stream and visualize the NBR TIFF files\n",
    "for year, files in nbr_files_130_by_year.items():\n",
    "    print(f\"Visualizing NBR files for year {year}...\")\n",
    "    for file_path in files:\n",
    "        print(f\"  Visualizing: {file_path}\")\n",
    "        try:\n",
    "            # Create a DataReference object\n",
    "            data_reference = DataReference(\n",
    "                datastore=datastore,\n",
    "                data_reference_name=\"landsat_data\",  # Choose a name \n",
    "                path_on_datastore=file_path\n",
    "            )\n",
    "\n",
    "            # Open the DataReference as a stream with rasterio\n",
    "            with rasterio.open(data_reference.as_download()) as dataset:\n",
    "                nbr_data = dataset.read(1)  # Read the NBR band\n",
    "\n",
    "                # Visualize the data\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.imshow(nbr_data, cmap='viridis')\n",
    "                plt.colorbar(label='Pixel Values')\n",
    "                plt.title(f'NBR Visualization: {file_path}')\n",
    "                plt.xlabel('Column')\n",
    "                plt.ylabel('Row')\n",
    "                plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "\n",
    "# Load the workspace configuration\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Access the default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Define the paths to your datasets\n",
    "landsat_054_path = \"UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001/\"\n",
    "landsat_130_path = \"UI/2024-11-26_040251_UTC/11022024-201615-130-20241126T031215Z-001/\"\n",
    "\n",
    "# Create datasets for the paths\n",
    "landsat_054_dataset = Dataset.File.from_files((datastore, landsat_054_path))\n",
    "landsat_130_dataset = Dataset.File.from_files((datastore, landsat_130_path))\n",
    "\n",
    "# Filter function for NBR files\n",
    "def filter_nbr_files(dataset):\n",
    "    file_list = dataset.to_path()\n",
    "    nbr_files = [file_path for file_path in file_list if \"NBR\" in file_path and \"NBR2\" not in file_path]\n",
    "    return nbr_files\n",
    "\n",
    "# Extract and analyze the data quality for NBR images\n",
    "def analyze_data_quality(nbr_files):\n",
    "    all_data = []\n",
    "    missing_files = 0\n",
    "    valid_dates = []\n",
    "\n",
    "    # Iterate over the NBR files to analyze them\n",
    "    for file in nbr_files:\n",
    "        try:\n",
    "            # Load the file into a numpy array using rasterio\n",
    "            with rasterio.open(file) as src:\n",
    "                data = src.read(1)  # Read the first band (NBR data)\n",
    "                if data is None or np.all(data == 0):\n",
    "                    missing_files += 1\n",
    "                    continue\n",
    "                valid_dates.append(file.split(\"/\")[-2])  # Extract the date from the file path\n",
    "                all_data.append(data)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "            missing_files += 1\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_data = np.array(all_data)\n",
    "\n",
    "    # Compute basic statistics for quality assessment\n",
    "    if len(all_data) > 0:\n",
    "        mean_values = np.mean(all_data, axis=(1, 2))  # Mean of each image\n",
    "        std_values = np.std(all_data, axis=(1, 2))  # Std deviation of each image\n",
    "        \n",
    "        return mean_values, std_values, valid_dates, missing_files\n",
    "    else:\n",
    "        return None, None, None, missing_files\n",
    "\n",
    "# Process both datasets\n",
    "nbr_files_054 = filter_nbr_files(landsat_054_dataset)\n",
    "nbr_files_130 = filter_nbr_files(landsat_130_dataset)\n",
    "\n",
    "# Analyze data quality\n",
    "mean_values_054, std_values_054, valid_dates_054, missing_files_054 = analyze_data_quality(nbr_files_054)\n",
    "mean_values_130, std_values_130, valid_dates_130, missing_files_130 = analyze_data_quality(nbr_files_130)\n",
    "\n",
    "# Visualize results (Mean values over time)\n",
    "def plot_data_quality_trends(valid_dates, mean_values, std_values, dataset_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(valid_dates, mean_values, label=f'{dataset_name} Mean NBR', color='b', marker='o')\n",
    "    plt.fill_between(valid_dates, mean_values - std_values, mean_values + std_values, color='blue', alpha=0.2)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Mean NBR Value')\n",
    "    plt.title(f'{dataset_name} NBR Data Quality Over Time')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for Landsat 054 and Landsat 130\n",
    "if mean_values_054 is not None:\n",
    "    plot_data_quality_trends(valid_dates_054, mean_values_054, std_values_054, \"Landsat 054\")\n",
    "\n",
    "if mean_values_130 is not None:\n",
    "    plot_data_quality_trends(valid_dates_130, mean_values_130, std_values_130, \"Landsat 130\")\n",
    "\n",
    "# Report missing data\n",
    "print(f\"Missing NBR files in Landsat 054 dataset: {missing_files_054}\")\n",
    "print(f\"Missing NBR files in Landsat 130 dataset: {missing_files_130}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Helper function to group files by year\n",
    "def group_files_by_year(file_paths, dates):\n",
    "    files_by_year = defaultdict(list)\n",
    "    for i, (year, month) in enumerate(dates):\n",
    "        files_by_year[year].append(file_paths[i])\n",
    "    return files_by_year\n",
    "\n",
    "# Group NBR files by year\n",
    "nbr_files_by_year = group_files_by_year(nbr_files_054 + nbr_files_130, nbr_dates_054 + nbr_dates_130)\n",
    "\n",
    "# Visualize one GeoTIFF per year\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, (year, files) in enumerate(sorted(nbr_files_by_year.items())):\n",
    "    file_path = files[0]  # Pick the first file for the year\n",
    "    with rasterio.open(file_path) as src:\n",
    "        data = src.read(1)  # Read the first band\n",
    "        bounds = src.bounds  # Geo-referenced bounds\n",
    "        crs = src.crs  # Coordinate reference system\n",
    "\n",
    "    # Plot the data\n",
    "    plt.subplot(4, 3, i + 1)  # Adjust rows/columns as needed\n",
    "    plt.imshow(data, cmap=\"viridis\")\n",
    "    plt.colorbar(label=\"NBR Values\")\n",
    "    plt.title(f\"Year: {year}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Optionally, print GeoTIFF metadata (e.g., bounds, CRS)\n",
    "    print(f\"Year: {year}, File: {file_path}\")\n",
    "    print(f\"Bounds: {bounds}, CRS: {crs}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Authenticate and initialize Azure ML Client\n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
    "\n",
    "# Step 2: Define datasets\n",
    "datasets = [\"jasper-landsat-130\", \"jasper-landsat-054\"]\n",
    "\n",
    "# Step 3: Function to process NBR .tif files\n",
    "def process_tif_file(tif_path):\n",
    "    try:\n",
    "        # Open and read the .tif file\n",
    "        with rasterio.open(tif_path) as dataset:\n",
    "            nbr_data = dataset.read(1)  # Read the first band\n",
    "            print(f\"File Metadata for {os.path.basename(tif_path)}: {dataset.meta}\")\n",
    "\n",
    "        # Visualize the NBR data\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.title(f\"NBR Visualization: {os.path.basename(tif_path)}\")\n",
    "        plt.imshow(nbr_data, cmap=\"RdYlGn\", vmin=-1, vmax=1)  # Set NBR range\n",
    "        plt.colorbar(label=\"NBR Value\")\n",
    "        plt.xlabel(\"Column Index\")\n",
    "        plt.ylabel(\"Row Index\")\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {tif_path}: {e}\")\n",
    "\n",
    "# Step 4: Process each dataset\n",
    "for dataset_name in datasets:\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    \n",
    "    # Fetch dataset details from Azure Datastore\n",
    "    data_asset = ml_client.data.get(name=dataset_name, version=\"2\")  # Update version if needed\n",
    "    dataset_path = data_asset.path\n",
    "    \n",
    "    # List all files in the dataset\n",
    "    print(f\"Dataset path: {dataset_path}\")\n",
    "    tif_files = [\n",
    "        f\"{dataset_path}/{file}\" for file in os.listdir(dataset_path) if file.endswith(\".tif\")\n",
    "    ]\n",
    "    \n",
    "    # Process each .tif file\n",
    "    for tif_file in tif_files:\n",
    "        print(f\"Processing file: {tif_file}\")\n",
    "        process_tif_file(tif_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Create Datasets for Landsat 130 and 054\n",
    "landsat_130_dataset = Dataset.File.from_files(path=(datastore, landsat_130_ui_path))\n",
    "landsat_054_dataset = Dataset.File.from_files(path=(datastore, landsat_054_ui_path))\n",
    "\n",
    "# Download the files\n",
    "landsat_130_dataset.download(target_path='./landsat_130', overwrite=True)\n",
    "landsat_054_dataset.download(target_path='./landsat_054', overwrite=True)\n",
    "\n",
    "# Assuming your Landsat data is in a format like CSV or GeoJSON\n",
    "# Adjust the following code based on your actual data format\n",
    "\n",
    "# Read the downloaded files into pandas DataFrames\n",
    "landsat_130_df = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "for filename in os.listdir('./landsat_130'):\n",
    "    if filename.endswith('.csv'):  # Or your file extension\n",
    "        filepath = os.path.join('./landsat_130', filename)\n",
    "        df = pd.read_csv(filepath)  # Or pd.read_json(), etc.\n",
    "        landsat_130_df = pd.concat([landsat_130_df, df], ignore_index=True)\n",
    "\n",
    "landsat_054_df = pd.DataFrame()\n",
    "for filename in os.listdir('./landsat_054'):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join('./landsat_054', filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        landsat_054_df = pd.concat([landsat_054_df, df], ignore_index=True)\n",
    "\n",
    "# Now you have landsat_130_df and landsat_054_df as pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths\n",
    "blob_path = \"UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001/\"\n",
    "datastore_path = [(datastore, blob_path)]\n",
    "\n",
    "# Create a TabularDataset or FileDataset\n",
    "dataset = Dataset.File.from_files(path=datastore_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files locally for inspection\n",
    "local_paths = dataset.download(target_path=\"data\", overwrite=True)  # Returns a list of downloaded paths\n",
    "\n",
    "# List all downloaded files from the paths\n",
    "import os\n",
    "\n",
    "# Check each path for files\n",
    "for path in local_paths:\n",
    "    if os.path.isdir(path):  # If it's a directory, list files inside\n",
    "        print(f\"Files in directory {path}:\")\n",
    "        print(os.listdir(path))\n",
    "    else:  # If it's a file, just print the file path\n",
    "        print(f\"Downloaded file: {path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter .tif files\n",
    "tif_files = [file for file in local_path if file.endswith('.tif')]\n",
    "print(tif_files[:10])  # Print the first 10 .tif files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total files: {len(local_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count file extensions\n",
    "extensions = [os.path.splitext(file)[1] for file in local_path]\n",
    "extension_counts = Counter(extensions)\n",
    "print(extension_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mltable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-ai-ml\n",
    "!pip install azure-identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient, command, Input\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Initialize MLClient\n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
    "\n",
    "# Get your dataset\n",
    "data_asset = ml_client.data.get(\"jasper-landsat-054\", version=\"1\")\n",
    "\n",
    "# Define the job command to list TIFF files\n",
    "job = command(\n",
    "    command=\"find ${inputs.data} -type f -name '*.tif'\",\n",
    "    inputs={\n",
    "        \"data\": Input(\n",
    "            path=data_asset.id,\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "            mode=InputOutputModes.RO_MOUNT,\n",
    "        ),\n",
    "    },\n",
    "    environment=\"azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
    ")\n",
    "\n",
    "# Submit the job\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "\n",
    "# Print job details\n",
    "print(f\"Job submitted. ID: {returned_job.id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Assuming `local_path` is the list returned by dataset.download()\n",
    "local_paths = dataset.download(target_path=\"data\", overwrite=True)\n",
    "\n",
    "# Iterate through each path in the list and print the files\n",
    "for path in local_paths:\n",
    "    if os.path.isdir(path):  # Check if the path is a directory\n",
    "        print(f\"Contents of {path}:\")\n",
    "        print(os.listdir(path))\n",
    "    else:\n",
    "        print(f\"File: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List all files\n",
    "files = [path for path in local_paths if os.path.isfile(path)]\n",
    "\n",
    "# Debugging: Print the first 5 files for inspection\n",
    "print(f\"Total files found: {len(files)}\")\n",
    "print(\"Sample file paths:\")\n",
    "for file in files[:5]:  # Print a sample\n",
    "    print(file)\n",
    "\n",
    "# Updated regex to capture the correct date portion (e.g., 20220708 from _20220708_)\n",
    "date_pattern = re.compile(r\"_(\\d{4})(\\d{2})(\\d{2})_\")  # Matches _YYYYMMDD_\n",
    "\n",
    "# Function to filter files by year\n",
    "def filter_files_by_year(files, year):\n",
    "    filtered_files = []\n",
    "    for file in files:\n",
    "        match = date_pattern.search(file)\n",
    "        if match:\n",
    "            # Extract the year, month, and day from the date\n",
    "            file_year = match.group(1)  # Extract YYYY\n",
    "            file_month = match.group(2)  # Extract MM (optional for future use)\n",
    "            file_day = match.group(3)  # Extract DD (optional for future use)\n",
    "\n",
    "            # Debug: Print matched years and full dates\n",
    "            print(f\"Matched date: {file_year}-{file_month}-{file_day} for file: {file}\")\n",
    "\n",
    "            if file_year == year:\n",
    "                filtered_files.append(file)\n",
    "    return filtered_files\n",
    "\n",
    "# Visualize raster files for a given year\n",
    "def visualize_files_by_year(files, year):\n",
    "    print(f\"\\nVisualizing data for the year {year}:\")\n",
    "    year_files = filter_files_by_year(files, year)\n",
    "    if not year_files:\n",
    "        print(f\"No files found for the year {year}.\")\n",
    "        return\n",
    "    \n",
    "    for file in year_files:\n",
    "        try:\n",
    "            with rasterio.open(file) as src:\n",
    "                data = src.read(1)  # Read the first band\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.imshow(data, cmap='viridis')\n",
    "                plt.colorbar(label=\"Pixel Values\")\n",
    "                plt.title(f\"Visualization of {os.path.basename(file)}\")\n",
    "                plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process file {file}: {e}\")\n",
    "\n",
    "# Loop through the years 2020–2024 and visualize\n",
    "years = [\"2020\", \"2021\", \"2022\", \"2023\", \"2024\"]\n",
    "for year in years:\n",
    "    visualize_files_by_year(files, year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "import os\n",
    "import re\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Connect to Azure ML Workspace\n",
    "workspace = Workspace.from_config()\n",
    "\n",
    "# Step 2: Access Datastore\n",
    "datastore = Datastore.get(workspace, 'workspaceblobstore')\n",
    "\n",
    "# Step 3: Define a single path for the NDVI files\n",
    "relative_path = 'UI/2024-11-19_143459_UTC/11022024-201612-054-20241119T143214Z-001/'  # Adjust if necessary\n",
    "\n",
    "# Step 4: Access dataset\n",
    "ndvi_dataset = Dataset.File.from_files((datastore, relative_path))\n",
    "\n",
    "# Step 5: Download dataset locally\n",
    "local_path = ndvi_dataset.download(target_path='.', overwrite=True)\n",
    "\n",
    "# Step 6: Collect all files from the downloaded path\n",
    "all_files = []\n",
    "for root, _, files in os.walk(local_path):\n",
    "    for file in files:\n",
    "        all_files.append(os.path.join(root, file))\n",
    "\n",
    "# Step 7: Filter files for NDVI in June, July, August 2023\n",
    "pattern = re.compile(r\".*NDVI.*_2023(06|07|08)_.*\\.tif$\")\n",
    "filtered_files = [file for file in all_files if pattern.search(file)]\n",
    "\n",
    "# Debugging: Check filtered files\n",
    "print(f\"Total NDVI files for June, July, August 2023: {len(filtered_files)}\")\n",
    "for file in filtered_files:\n",
    "    print(f\"Matched file: {file}\")\n",
    "\n",
    "# Step 8: Process and visualize NDVI files\n",
    "if not filtered_files:\n",
    "    print(\"No NDVI files found for the specified criteria.\")\n",
    "else:\n",
    "    for file_path in filtered_files:\n",
    "        try:\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            with rasterio.open(file_path) as src:\n",
    "                # Read NDVI data\n",
    "                data = src.read(1)  # Assuming first band contains NDVI\n",
    "                mean_value = np.nanmean(data)\n",
    "                print(f\"Mean NDVI value: {mean_value}\")\n",
    "                \n",
    "                # Visualize NDVI data\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.imshow(data, cmap='viridis')\n",
    "                plt.colorbar(label='NDVI Values')\n",
    "                plt.title(f\"NDVI Visualization: {os.path.basename(file_path)}\")\n",
    "                plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDVI \n",
    "\n",
    "##### (NDVI) Normalized Difference Vegetation Index, a remote sensing method that measures the health and amount of vegetation in an area\n",
    "\n",
    "##### Burn Scar 2022\n",
    "\n",
    "![NDVI Burn Scar](https://firerecovery.blob.core.windows.net/azureml-blobstore-d516a889-2242-4cd1-9ee4-4a3315f1782b/UI/2024-11-30_192011_UTC/photos/photos/photos/photos/Screenshot%202024-11-30%20150106.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Discussion\n",
    "###### As a group we learned a significant amount about how to use Azure Storage Blob, Azure Machine Learning Studio, machine learning models, and geospatial packages while better understanding forest fire and the long process of forest recovery that in this case demonstrated that it was not a natural regrowth but required careful, diligent, and constrained long-range intervention. \n",
    "\n",
    "###### The TIFF files are incredibly large so it took some time just to learn how to acquire and bring them into the Azure Storage Blob. \n",
    "###### \n",
    "###### Additional challenges included getting library dependencies to work together and resolve countless errors. \n",
    "\n",
    "###### There is much potential for the use of machine learning tools such as Azure AI to explore satellite imagery and various indices (NDVI, NBR) and do various analysis to understand burn severity, recovery, and change over time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "\n",
    "####  Takeaways: Key Insights for the Jasper Fire Recovery\n",
    "\n",
    "###### * Understanding the long-term recovery process highlights the need for continued innovation in restoration and resilience planning.\n",
    "###### * Satellite imagery and AI-driven analysis enable real-time tracking of vegetation regrowth, land use changes, and fire-prone areas.\n",
    "###### * Manual replanting is essential for forest restoration, but it is constrained by time and resource limits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "###### Chen, X., Vogelmann, J. E., Rollins, M., Ohlen, D., Key, C. H., Yang, L., Huang, C., & Shi, H. (2011). Detecting post-fire burn severity and vegetation recovery using multitemporal remote sensing spectral indices and field-collected composite burn index data in a ponderosa pine forest. International Journal of Remote Sensing, 32(23), 7905–7927. https://doi.org/10.1080/01431161.2010.524678\n",
    "###### ● Crookston, N. L., & Dixon, G. E. (2005). The forest vegetation simulator: A review of its structure, content, and applications. Computers and Electronics in Agriculture, 49, 60–80. https://doi.org/10.1016/j.compag.2005.08.005\n",
    "###### ● Ex, S. A., Smith, F. W., Keyser, T. L., & Rebain, S. A. (2016). Development and evaluation of equations for estimating canopy fuel characteristics for 10 major conifers of the western United States. Western Journal of Applied Forestry, 31(4), 161–169. https://doi.org/10.5849/wjaf.15-076\n",
    "###### ● Fulé, P. Z., Waltz, A. E. M., Covington, W. W., & Heinlein, T. A. (2001). Measuring forest restoration effectiveness in reducing hazardous fuels. Journal of Forestry, 99(1), 24–29. https://doi.org/10.1093/jof/99.1.24\n",
    "###### ● Hawley, C. M., Loudermilk, E. L., Rowell, E. M., & Pokswinski, S. (2018). A novel approach to 3D fuels biomass sampling for 3D fuel characterization. MethodsX, 5, 1597–1604. https://doi.org/10.1016/j.mex.2018.10.002\n",
    "###### ● Keyser, T., Smith, F. W., & Shepperd, W. (2009). Estimating canopy fuels and their impact on potential fire behavior for ponderosa pine in the Black Hills, South Dakota. DigitalCommons@University of Nebraska - Lincoln. https://digitalcommons.unl.edu/jfspresearch/138\n",
    "###### ● Maxwell, A. E., Gallagher, M. R., Minicuci, N., Bester, M. S., Loudermilk, E. L., Pokswinski, S. M., & Skowronski, N. S. (2023). Impact of reference data sampling density for estimating plot-level average shrub heights using terrestrial laser scanning data. Fire, 6(3), 98. https://doi.org/10.3390/fire6030098\n",
    "###### ● Parks, S. A., Dobrowski, S. Z., Parisien, M.-A., Miller, C., & Hudak, A. T. (2022). Burn severity mapping: A synthesis of the state of the science and a look to the future. International Journal of Wildland Fire, 31, 539–560. https://doi.org/10.1071/WF22050\n",
    "###### ● Picotte, J. J., Cansler, C. A., Kolden, C. A., Lutz, J. A., Key, C., Benson, N. C., & Robertson, K. M. (2021). Development of nationally consistent and ecologically relevant fire severity metrics from Landsat time series. Remote Sensing of Environment, 263, 112569. https://doi.org/10.1016/j.rse.2021.112569\n",
    "###### ● Reiner, A. L., Baker, C., Wahlberg, M., Rau, B. M., & Birch, J. D. (2022). Region-specific remote-sensing models for predicting burn severity, basal area change, and canopy cover change following fire in the southwestern United States. Fire, 5(5), 137. https://doi.org/10.3390/fire5050137\n",
    "###### ● Ross, C. W., Loudermilk, E. L., Skowronski, N., Pokswinski, S., Hiers, J. K., & O’Brien, J. (2022). LiDAR voxel-size optimization for canopy gap estimation. Remote Sensing, 14(5), 1054. https://doi.org/10.3390/rs14051054\n",
    "###### ● Rowell, E., Loudermilk, E. L., Pokswinski, S., Hiers, J., O’Brien, J., Mathey, J., & Robertson, K. (2020). Coupling terrestrial laser scanning with 3D fuel biomass sampling for advancing wildland fuels characterization. Forest Ecology and Management, 462, 117945. https://doi.org/10.1016/j.foreco.2020.117945\n",
    "###### ● Scheffler, D., & Frantz, D. (2022). Regionally optimized spectral harmonization of Landsat-5, Landsat-7, and Landsat-8 for improved forest monitoring in the Bavarian Forest National Park, Germany. International Journal of Applied Earth Observation and Geoinformation, 115, 103126. https://doi.org/10.1016/j.jag.2022.103126\n",
    "###### ● Smith, A. M. S., Lentile, L. B., Hudak, A. T., & Morgan, P. (2007). Evaluation of linear spectral unmixing and ΔNBR for predicting post-fire recovery in a North American ponderosa pine forest. International Journal of Remote Sensing, 28(22), 5159–5166. https://doi.org/10.1080/01431160701395161\n",
    "###### ● U.S. Department of Agriculture, Forest Service. (2003). Fire and Fuels Extension to the Forest Vegetation Simulator. https://www.fs.fed.us/fmsc/fvs/FFEguide.pdf\n",
    "###### ● Xi, Z., Chasmer, L., & Hopkinson, C. (2023). Delineating and reconstructing 3D forest fuel components and volumes with terrestrial laser scanning. Remote Sensing, 15(19), 4778. https://doi.org/10.3390/rs15194778\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contact\n",
    "###### For any inquiries, please reach out to the team:\n",
    "###### \n",
    "###### Manuel Malla: manuel.malla@studentambassadors.com\n",
    "###### Yash Padhara: Yash.Padhara@studentambassadors.com\n",
    "###### Sneha Pandey: sneha.pandey@studentambassadors.com\n",
    "###### Philippa Burgess: philippa.burgess@studentambassadors.com"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
